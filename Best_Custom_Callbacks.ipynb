{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Best Custom Callbacks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNqe0hLA+DWB8t0xYk/v+z1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/Kaggle-Kernels/blob/master/Best_Custom_Callbacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znAO8dny7XAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm \n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, LearningRateScheduler, TensorBoard, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.layers import Conv2D, SpatialDropout1D, BatchNormalization, MaxPool2D, Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, Nadam\n",
        "from tensorflow.keras.applications.nasnet import NASNetMobile, preprocess_input\n",
        "from tensorflow.keras.datasets import cifar10, mnist\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDkG8nQS8Adm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(Sequence):\n",
        "  def __init__(self, images, labels, batch_size=64, image_dimension=(32,32,3), shuffle=False, augment=False):\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.dim = image_dimension\n",
        "    self.shuffle = shuffle\n",
        "    self.augment = augment\n",
        "    self.on_epoch_end()\n",
        "  \n",
        "  def __len__(self):\n",
        "    return int(np.floor(len(self.images)/self.batch_size))\n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(len(self.images))\n",
        "    if self.shuffle:\n",
        "      return np.random.shuffle(self.indexes)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    indexes = self.indexes[index * self.batch_size : (index+1) * self.batch_size]\n",
        "    labels = self.labels[indexes]\n",
        "    images = self.images[indexes,:,:,:]        \n",
        "    if self.augment==True:\n",
        "      print(\"Data Augmentation in Progress ..\")\n",
        "      images = self.augmentor(self.images)\n",
        "      print(\"Done.\")\n",
        "    images = images/255.0\n",
        "    return images, labels\n",
        "  \n",
        "  def augmentor(self, images):\n",
        "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "    seq = iaa.Sequential(\n",
        "        [\n",
        "        iaa.Fliplr(0.5),  \n",
        "        iaa.Flipud(0.2),  \n",
        "        sometimes(iaa.Affine(\n",
        "          scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
        "          translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
        "          rotate=(-10, 10),\n",
        "          shear=(-5, 5), \n",
        "          order=[0, 1],\n",
        "          cval=(0, 255),\n",
        "          mode=ia.ALL\n",
        "        )),\n",
        "        iaa.SomeOf((0, 5),\n",
        "                    [sometimes(iaa.Superpixels(p_replace=(0, 1.0),\n",
        "                                                  n_segments=(20, 200))),\n",
        "                      iaa.OneOf([\n",
        "                          iaa.GaussianBlur((0, 1.0)),\n",
        "                          iaa.AverageBlur(k=(3, 5)),\n",
        "                          iaa.MedianBlur(k=(3, 5)),\n",
        "                      ]),\n",
        "                      iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)),\n",
        "                      iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
        "                      iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "                          iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
        "                          iaa.DirectedEdgeDetect(alpha=(0.5, 1.0),\n",
        "                                                direction=(0.0, 1.0)),\n",
        "                      ])),\n",
        "                      iaa.AdditiveGaussianNoise(loc=0,\n",
        "                                                scale=(0.0, 0.01 * 255),\n",
        "                                                per_channel=0.5),\n",
        "                      iaa.OneOf([\n",
        "                          iaa.Dropout((0.01, 0.05), per_channel=0.5),\n",
        "                          iaa.CoarseDropout((0.01, 0.03),\n",
        "                                            size_percent=(0.01, 0.02),\n",
        "                                            per_channel=0.2),\n",
        "                      ]),\n",
        "                      iaa.Invert(0.01, per_channel=True),\n",
        "                      iaa.Add((-2, 2), per_channel=0.5),\n",
        "                      iaa.AddToHueAndSaturation((-1, 1)),\n",
        "                      iaa.OneOf([\n",
        "                          iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "                          iaa.FrequencyNoiseAlpha(\n",
        "                              exponent=(-1, 0),\n",
        "                              first=iaa.Multiply((0.9, 1.1),\n",
        "                                                per_channel=True),\n",
        "                              second=iaa.ContrastNormalization(\n",
        "                                  (0.9, 1.1))\n",
        "                          )\n",
        "                      ]),\n",
        "                      sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5),\n",
        "                                                          sigma=0.25)),\n",
        "                      sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n",
        "                      sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
        "                    ],\n",
        "                    random_order=True\n",
        "                    )\n",
        "        ],\n",
        "        random_order=True\n",
        "    )\n",
        "    return seq.augment_images(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdAIM059CKJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTModel():\n",
        "  def __init__(self, image_dimension=(32,32,3),model=1,n_classes=10,version=2, depth=11):\n",
        "    self.n_classes = n_classes\n",
        "    self.input_dim = image_dimension \n",
        "    self.version = version\n",
        "    self.depth = depth\n",
        "    if model==1:\n",
        "      self.model = self.create_simple()\n",
        "    else:\n",
        "      self.model = self.create_model()\n",
        "    model_type = 'ResNet%dv%d' % (self.depth, self.version)\n",
        "    print(\"Model: \", model_type)\n",
        "  \n",
        "  def summary(self):\n",
        "    self.model.summary()\n",
        "\n",
        "  def create_simple(self):\n",
        "    input_layer = Flatten((32,32,3))\n",
        "    x = Dense(768,activation='relu')(x)\n",
        "    output_layer = Dense(self.n_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=0.0005), loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
        "    return model\n",
        "  def resnet_layer(self,inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "      conv = Conv2D(num_filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    strides=strides,\n",
        "                    padding='same',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=l2(1e-4))\n",
        "\n",
        "      x = inputs\n",
        "      if conv_first:\n",
        "          x = conv(x)\n",
        "          if batch_normalization:\n",
        "              x = BatchNormalization()(x)\n",
        "          if activation is not None:\n",
        "              x = Activation(activation)(x)\n",
        "      else:\n",
        "          if batch_normalization:\n",
        "              x = BatchNormalization()(x)\n",
        "          if activation is not None:\n",
        "              x = Activation(activation)(x)\n",
        "          x = conv(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "  def resnet_v1(self, input_shape, depth, num_classes=10):\n",
        "      if (depth - 2) % 6 != 0:\n",
        "          raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "      num_filters = 16\n",
        "      num_res_blocks = int((depth - 2) / 6)\n",
        "      inputs = Input(shape=input_shape)\n",
        "      x = self.resnet_layer(inputs=inputs)\n",
        "      for stack in range(3):\n",
        "          for res_block in range(num_res_blocks):\n",
        "              strides = 1\n",
        "              if stack > 0 and res_block == 0: \n",
        "                  strides = 2  \n",
        "              y = self.resnet_layer(inputs=x,\n",
        "                              num_filters=num_filters,\n",
        "                              strides=strides)\n",
        "              y = self.resnet_layer(inputs=y,\n",
        "                              num_filters=num_filters,\n",
        "                              activation=None)\n",
        "              if stack > 0 and res_block == 0:\n",
        "                  x = self.resnet_layer(inputs=x,\n",
        "                                  num_filters=num_filters,\n",
        "                                  kernel_size=1,\n",
        "                                  strides=strides,\n",
        "                                  activation=None,\n",
        "                                  batch_normalization=False)\n",
        "              x = keras.layers.add([x, y])\n",
        "              x = Activation('relu')(x)\n",
        "          num_filters *= 2\n",
        "      x = AveragePooling2D(pool_size=8)(x)\n",
        "      y = Flatten()(x)\n",
        "      outputs = Dense(num_classes,\n",
        "                      activation='softmax',\n",
        "                      kernel_initializer='he_normal')(y)\n",
        "      model = Model(inputs=inputs, outputs=outputs)\n",
        "      return model\n",
        "\n",
        "\n",
        "  def resnet_v2(self,input_shape, depth, num_classes=10):\n",
        "      if (depth - 2) % 9 != 0:\n",
        "          raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "      num_filters_in = 16\n",
        "      num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "      inputs = Input(shape=input_shape)\n",
        "      x = self.resnet_layer(inputs=inputs,\n",
        "                      num_filters=num_filters_in,\n",
        "                      conv_first=True)\n",
        "      for stage in range(3):\n",
        "          for res_block in range(num_res_blocks):\n",
        "              activation = 'relu'\n",
        "              batch_normalization = True\n",
        "              strides = 1\n",
        "              if stage == 0:\n",
        "                  num_filters_out = num_filters_in * 4\n",
        "                  if res_block == 0: \n",
        "                      activation = None\n",
        "                      batch_normalization = False\n",
        "              else:\n",
        "                  num_filters_out = num_filters_in * 2\n",
        "                  if res_block == 0:  \n",
        "                      strides = 2    \n",
        "\n",
        "              y = self.resnet_layer(inputs=x,\n",
        "                              num_filters=num_filters_in,\n",
        "                              kernel_size=1,\n",
        "                              strides=strides,\n",
        "                              activation=activation,\n",
        "                              batch_normalization=batch_normalization,\n",
        "                              conv_first=False)\n",
        "              y = self.resnet_layer(inputs=y,\n",
        "                              num_filters=num_filters_in,\n",
        "                              conv_first=False)\n",
        "              y = self.resnet_layer(inputs=y,\n",
        "                              num_filters=num_filters_out,\n",
        "                              kernel_size=1,\n",
        "                              conv_first=False)\n",
        "              if res_block == 0:\n",
        "                  x = self.resnet_layer(inputs=x,\n",
        "                                  num_filters=num_filters_out,\n",
        "                                  kernel_size=1,\n",
        "                                  strides=strides,\n",
        "                                  activation=None,\n",
        "                                  batch_normalization=False)\n",
        "              x = keras.layers.add([x, y])\n",
        "\n",
        "          num_filters_in = num_filters_out\n",
        "\n",
        "      x = BatchNormalization()(x)\n",
        "      x = Activation('relu')(x)\n",
        "      x = AveragePooling2D(pool_size=8)(x)\n",
        "      y = Flatten()(x)\n",
        "      outputs = Dense(num_classes,\n",
        "                      activation='softmax',\n",
        "                      kernel_initializer='he_normal')(y)\n",
        "\n",
        "      model = Model(inputs=inputs, outputs=outputs)\n",
        "      return model\n",
        "\n",
        "  def create_model(self):\n",
        "    if self.version == 2:\n",
        "        model = self.resnet_v2(input_shape=self.input_dim, depth=self.depth, num_classes=self.n_classes)\n",
        "    else:\n",
        "        model = self.resnet_v1(input_shape=self.input_dim, depth=self.depth, num_classes=self.n_classes)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=self.lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def lr_schedule(self, epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "    \n",
        "  def train(self, train_data, val_data, plot_results=True):\n",
        "    print(\"Starting Training\")\n",
        "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                patience=2,\n",
        "                                                verbose=1,\n",
        "                                                factor=0.5,\n",
        "                                                min_lr=0.000000001)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                   patience=5,\n",
        "                                   mode=\"min\",\n",
        "                                   restore_best_weights=True,\n",
        "                                   verbose=1)\n",
        "    \n",
        "    checkpoint = ModelCheckpoint('./model_weights.hdf5',\n",
        "                                 monitor='val_loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 mode='min',\n",
        "                                 save_weights_only=True)\n",
        "    \n",
        "    history = self.model.fit_generator(generator=train_data,\n",
        "\t\t                                   validation_data=val_data,\n",
        "\t\t                                   epochs=EPOCHS,\n",
        "\t\t                                   steps_per_epoch=len(train_data),\n",
        "\t\t                                   validation_steps =len(val_data),\n",
        "\t\t                                   callbacks=[learning_rate_reduction, early_stopping, checkpoint],\n",
        "\t\t                                   verbose=1,\n",
        "\t\t                                   )\n",
        "    if plot_results:\n",
        "      fig, ax = plt.subplots(2, 1, figsize=(6, 6))\n",
        "      ax[0].plot(history.history['loss'], label=\"TrainLoss\")\n",
        "      ax[0].plot(history.history['val_loss'], label=\"ValLoss\")\n",
        "      ax[0].legend(loc='best', shadow=True)\n",
        "\n",
        "      ax[1].plot(history.history['acc'], label=\"TrainAcc\")\n",
        "      ax[1].plot(history.history['val_acc'], label=\"ValAcc\")\n",
        "      ax[1].legend(loc='best', shadow=True)\n",
        "      plt.show()\n",
        "   \n",
        "  def create_submit(self, test_data):\n",
        "    'Create basic file submit'\n",
        "    self.model.load_weights(\"./model_weights.hdf5\")\n",
        "    results = self.model.predict_generator(test_data)\n",
        "    results_to_save = pd.DataFrame({\"id\": test_data.images_paths,\n",
        "                                    \"label\": results[:,0]\n",
        "                                    })\n",
        "\n",
        "    results_to_save[\"id\"] = results_to_save[\"id\"].apply(lambda x: x.replace(\"../input/test/\", \"\").replace(\".tif\", \"\"))\n",
        "    results_to_save.to_csv(\"./submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBueocnYEr8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def loadData(db, val_split=0.2,sub_sample_size=-1):\n",
        "  (X_train, y_train), (X_test,y_test) = cifar10.load_data()\n",
        "  if db == \"train\":\n",
        "    if val_split>0:\n",
        "      X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=val_split,stratify=y_train)\n",
        "      print(\"Train Data completed\")\n",
        "      train_data = DataGenerator(X_train, y_train, batch_size=BATCH_SIZE,image_dimension=(32,32,3), shuffle=True, augment=False)\n",
        "      print(\"val Data completed\")\n",
        "      val_data = DataGenerator(X_val, y_val, batch_size=BATCH_SIZE,image_dimension=(32,32,3), shuffle=False, augment=False)\n",
        "      return train_data, val_data\n",
        "    else:\n",
        "      return DataGenerator(X_train,y_train,batch_size=BATCH_SIZE,image_dimension=(32,32,3),augment=True,shuffle=True), None  \n",
        "  else:\n",
        "    if sub_sample_size== -1 :\n",
        "      return DataGenerator(X_test, y_test, batch_size=2), None\n",
        "    else:\n",
        "      return DataGenerator(X_test[:sub_sample_size], y_test[:sub_sample_size], batch_size=1), None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxT72NSQJzXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\tEPOCHS = 10\n",
        "\tBATCH_SIZE = 64\n",
        "\tIMAGE_DIMENSIONS = (32,32 ,1)\n",
        "\n",
        "\tmodel = MNISTModel(IMAGE_DIMENSIONS,model=1,n_classes=10,version=2,depth=11)\n",
        "\tmodel.summary()\n",
        "\n",
        "\t# train model\n",
        "\ttrain_data, val_data = loadData(\"train\", val_split=0.2)\n",
        "\tmodel.train(train_data, val_data, plot_results=True)\n",
        "\n",
        "\t# submit model\n",
        "\ttest_data, _ = loadData(\"test\")\n",
        "\tmodel.create_submit(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE6GV-uDKmfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/c/quickdraw-doodle-recognition/discussion/71760\n",
        "# https://www.kaggle.com/rejpalcz/cnn-128x128x4-keras-from-scratch-lb-0-328\n",
        "# https://www.kaggle.com/mpalermo/keras-pipeline-custom-generator-imgaug\n",
        "# https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/73929\n",
        "# https://www.kaggle.com/fergusoci/keras-loss-based-learning-rate-scheduler\n",
        "# https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807\n",
        "# https://www.kaggle.com/lavanyashukla01/better-models-faster-with-weights-biases\n",
        "# https://www.kaggle.com/kozistr/0-99757-lb-top-4-with-keras\n",
        "# https://www.kaggle.com/gpiosenka/tf2-keras-cnn-with-custom-callbacks\n",
        "# https://www.kaggle.com/ashhafez/keras-custom-learning-rate-decay\n",
        "# https://www.kaggle.com/danmoller/make-best-use-of-a-kernel-s-limited-uptime-keras\n",
        "# https://www.kaggle.com/residentmario/keras-callbacks-and-config-files\n",
        "# https://github.com/analyticsindiamagazine/MachineHack/blob/master/Hackathon_Solutions/Chartbusters%20Prediction%20:%20Foretell%20The%20Popularity%20Of%20Songs/Rank_1_Nikhil/best_model.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VTMQQCRScJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}